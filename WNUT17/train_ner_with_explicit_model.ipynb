{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_ner.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp_eval",
      "language": "python",
      "name": "nlp_eval"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoMangiavacchi/NER_Training/blob/main/WNUT17/train_ner_with_explicit_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiO4F03tP2kC"
      },
      "source": [
        "# Install Transformers for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3V7MXxM7P0",
        "outputId": "ed700711-f921-49f5-9194-3d139b92925c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgJyCvrPP_2-"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR14T8v84PNx"
      },
      "source": [
        "PRE_TRAINED_BERT_MODEL = 'distilbert-base-cased'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd9w21AzMr3V"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from transformers.activations import gelu\n",
        "from transformers import DistilBertConfig\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "\n",
        "from transformers.modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "from transformers import PreTrainedModel\n",
        "\n",
        "from transformers import DistilBertPreTrainedModel\n",
        "\n",
        "from transformers import DistilBertModel\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsoRPO8puE3r"
      },
      "source": [
        "# Distilbert For Token Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4HOHERJtyZ7"
      },
      "source": [
        "class DistilBertForTokenClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9N4MbV30FYF"
      },
      "source": [
        "# Get Distilbert for Token Classification model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PM6Bqv90Ewu",
        "outputId": "27191dfb-c47d-4e66-93dc-0d1f5aabd3da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from transformers import DistilBertForTokenClassification\n",
        "\n",
        "unique_tags = {\n",
        " 'B-group',\n",
        " 'B-product',\n",
        " 'I-group',\n",
        " 'B-person',\n",
        " 'I-product',\n",
        " 'B-corporation',\n",
        " 'I-corporation',\n",
        " 'I-creative-work',\n",
        " 'B-creative-work',\n",
        " 'I-location',\n",
        " 'O',\n",
        " 'B-location',\n",
        " 'I-person'\n",
        "}    \n",
        "\n",
        "model = DistilBertForTokenClassification.from_pretrained(PRE_TRAINED_BERT_MODEL, num_labels=len(unique_tags))\n",
        "\n",
        "torch.save(model.state_dict(), PRE_TRAINED_BERT_MODEL + '_weight.pth')\n",
        "\n",
        "del model"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AjNHs58ECv0"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcwgb4LECv2",
        "outputId": "0d14a9b9-4992-4d19-b8ff-6a4fd61cef46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-06 12:18:57--  http://noisy-text.github.io/2017/files/wnut17train.conll\n",
            "Resolving noisy-text.github.io (noisy-text.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\n",
            "Connecting to noisy-text.github.io (noisy-text.github.io)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493781 (482K) [application/octet-stream]\n",
            "Saving to: ‘wnut17train.conll.1’\n",
            "\n",
            "\rwnut17train.conll.1   0%[                    ]       0  --.-KB/s               \rwnut17train.conll.1 100%[===================>] 482.21K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-11-06 12:18:58 (8.86 MB/s) - ‘wnut17train.conll.1’ saved [493781/493781]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl2WEc8zECv8"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def read_wnut(file_path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    raw_text = file_path.read_text().strip()\n",
        "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for line in doc.split('\\n'):\n",
        "            token, tag = line.split('\\t')\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs\n",
        "\n",
        "texts, tags = read_wnut('wnut17train.conll')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsnoh5HmECwA",
        "outputId": "d598cb4e-4924-4ec4-aaca-8942b6ff496e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(set([str(t) for line in tags for t in line]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'B-product',\n",
              " 'B-corporation',\n",
              " 'I-person',\n",
              " 'I-location',\n",
              " 'I-product',\n",
              " 'B-person',\n",
              " 'B-location',\n",
              " 'I-group',\n",
              " 'I-corporation',\n",
              " 'B-group',\n",
              " 'I-creative-work',\n",
              " 'B-creative-work']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCVoSsOIECwE",
        "outputId": "9dc11ac7-6081-4267-c7e2-7c353cb0c28b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
            "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTr96XeECwG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e13IwtJzECwJ",
        "outputId": "0003910b-c430-4e60-fa5c-6d7c0ae45bf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_texts), len(val_texts)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUwIwdr8ECwM"
      },
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs44h6xyECwO",
        "outputId": "e39dd629-c982-4171-f28d-e1c3c04d5572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "id2tag"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-product',\n",
              " 2: 'B-corporation',\n",
              " 3: 'I-person',\n",
              " 4: 'I-location',\n",
              " 5: 'I-product',\n",
              " 6: 'B-person',\n",
              " 7: 'B-location',\n",
              " 8: 'I-group',\n",
              " 9: 'I-corporation',\n",
              " 10: 'B-group',\n",
              " 11: 'I-creative-work',\n",
              " 12: 'B-creative-work'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2jIJICiECwQ"
      },
      "source": [
        "# Use pre-trained DistilBert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxgof0hcECwQ"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_BERT_MODEL)\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SzEbadHECwS",
        "outputId": "19001b4a-3d3d-4f1f-8125-090290ea0747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encodings.keys()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAibb_vECwU",
        "outputId": "b5107aa6-5c15-487b-f724-36d1163f63d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_encodings['input_ids']), len(val_encodings['input_ids'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dax_NmvvECwW",
        "outputId": "4518b27b-7efd-4293-b1f2-b6fb3a24c0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_encodings['input_ids'][0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FxJ8bDnECwZ"
      },
      "source": [
        "# Format labels for sub token splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMIN-NmWECwZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VED-SYcGECwb",
        "outputId": "d490a382-deca-4910-8cd6-da8937b862ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_labels), len(val_labels)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWJULml-ECwd",
        "outputId": "ecc0c266-8bf2-4bca-aecb-2d07ca7a96d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_labels[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3Bm9jBECwf"
      },
      "source": [
        "# Create Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67IxZZXECwf"
      },
      "source": [
        "import torch\n",
        "\n",
        "class WNUTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
        "val_dataset = WNUTDataset(val_encodings, val_labels)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSGqeUW0zOf"
      },
      "source": [
        "# Instantiate Model from PyTorch class module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjlIDyx83Bry"
      },
      "source": [
        "from transformers import BertConfig\n",
        "config = BertConfig.from_pretrained(PRE_TRAINED_BERT_MODEL, num_labels=len(unique_tags)) "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2E7erXs0zAp"
      },
      "source": [
        "model = DistilBertForTokenClassification(config)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55ZRJlO1AfL"
      },
      "source": [
        "# Load Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7t8eZj61Ast"
      },
      "source": [
        "weight = torch.load(PRE_TRAINED_BERT_MODEL + '_weight.pth', map_location='cpu')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6MVFucy2Bar",
        "outputId": "6ce2050e-55d6-47d6-cbeb-6ee5f7418d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load_state_dict(weight)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_Xz_HaECwk"
      },
      "source": [
        "# Fine Tune DistilBert using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEqQDeUUECwk",
        "outputId": "d34ddc96-6817-4d00-a16e-6cc41a72ce97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(10):\n",
        "    epoch_loss = 0.0\n",
        "    num_batch = 0\n",
        "    for i, batch in enumerate(train_loader, 0):\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #print(f\"Epoch {epoch} Batch {i} : Loss {loss.data}\")\n",
        "        epoch_loss += loss.item()\n",
        "        num_batch += 1\n",
        "    print(f\"Epoch {epoch} : Loss {epoch_loss / num_batch}\")\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss 0.281829978591379\n",
            "Epoch 1 : Loss 0.1241909433484954\n",
            "Epoch 2 : Loss 0.062274123361224634\n",
            "Epoch 3 : Loss 0.031746697283404714\n",
            "Epoch 4 : Loss 0.017278146973352275\n",
            "Epoch 5 : Loss 0.010022455858758323\n",
            "Epoch 6 : Loss 0.00908677917519954\n",
            "Epoch 7 : Loss 0.005213445275320042\n",
            "Epoch 8 : Loss 0.004401053238047205\n",
            "Epoch 9 : Loss 0.004597072739852592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjIY3GyqS9bN"
      },
      "source": [
        "# Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDb-iNphECwn"
      },
      "source": [
        "testloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de32VjIDTBaP"
      },
      "source": [
        "predictions = []\n",
        "actual = []\n",
        "with torch.no_grad():\n",
        "   for i, batch in enumerate(testloader, 0):\n",
        "     input_ids = batch['input_ids'].to(device)\n",
        "     attention_mask = batch['attention_mask'].to(device)\n",
        "     outputs = model(input_ids, attention_mask)\n",
        "     predictions.append(outputs[0].cpu())\n",
        "     actual.append(batch['labels'])\n",
        "predictions = np.concatenate(predictions)\n",
        "actual = np.concatenate(actual)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vt3pDcUTBf_"
      },
      "source": [
        "predicted_tokens = np.argmax(predictions, axis=2)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzcQirVlTBq3"
      },
      "source": [
        "predicted_tags = []\n",
        "\n",
        "for p_tokens, a_tokens in zip(predicted_tokens, actual):\n",
        "  a_mask = a_tokens!=-100\n",
        "  word_tokens = p_tokens[a_mask]\n",
        "  iob_tags = [id2tag[x] for x in word_tokens]\n",
        "  predicted_tags.append(iob_tags)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5sbg_58TCUC",
        "outputId": "eb18072f-3144-43fd-d30e-ee27ec86dd3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 8.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp36-none-any.whl size=16171 sha256=357f75233127176ba8adba242cd76945ada2c1bab7fdd575349a2d4c93dedec3\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMmTRcGTBVh"
      },
      "source": [
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH2-QCiPECwp",
        "outputId": "eea4afa2-acbb-4dc1-8113-72bcde39eb7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f1_score(val_tags, predicted_tags)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.54014598540146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}