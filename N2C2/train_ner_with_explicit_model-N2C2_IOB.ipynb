{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_ner.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp_eval",
      "language": "python",
      "name": "nlp_eval"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JacopoMangiavacchi/NER_Training/blob/main/train_ner_with_explicit_model-N2C2_IOB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiO4F03tP2kC"
      },
      "source": [
        "# Install Transformers for Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR3V7MXxM7P0",
        "outputId": "53c089d9-a791-44c9-d2a5-eb29aa00d03f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgJyCvrPP_2-"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR14T8v84PNx"
      },
      "source": [
        "PRE_TRAINED_BERT_MODEL = 'distilbert-base-cased'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd9w21AzMr3V"
      },
      "source": [
        "import copy\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from transformers.activations import gelu\n",
        "from transformers import DistilBertConfig\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "\n",
        "from transformers.modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "\n",
        "from transformers.utils import logging\n",
        "\n",
        "from transformers import PreTrainedModel\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D37tgrEVtyZ5"
      },
      "source": [
        "# Basic Transformers Distilbert class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByoHEM0Rt-hZ"
      },
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CONFIG_FOR_DOC = \"DistilBertConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"DistilBertTokenizer\"\n",
        "\n",
        "DISTILBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"distilbert-base-uncased\",\n",
        "    \"distilbert-base-uncased-distilled-squad\",\n",
        "    \"distilbert-base-cased\",\n",
        "    \"distilbert-base-cased-distilled-squad\",\n",
        "    \"distilbert-base-german-cased\",\n",
        "    \"distilbert-base-multilingual-cased\",\n",
        "    \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    # See all DistilBERT models at https://huggingface.co/models?filter=distilbert\n",
        "]\n",
        "\n",
        "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
        "    position_enc = np.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])\n",
        "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "    out.detach_()\n",
        "    out.requires_grad = False\n",
        "\n",
        "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
        "    position_enc = np.array([[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)])\n",
        "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "    out.detach_()\n",
        "    out.requires_grad = False\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n",
        "        if config.sinusoidal_pos_embds:\n",
        "            create_sinusoidal_embeddings(\n",
        "                n_pos=config.max_position_embeddings, dim=config.dim, out=self.position_embeddings.weight\n",
        "            )\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_ids: torch.tensor(bs, max_seq_length)\n",
        "            The token ids to embed.\n",
        "\n",
        "        Outputs\n",
        "        -------\n",
        "        embeddings: torch.tensor(bs, max_seq_length, dim)\n",
        "            The embedded tokens (plus position embeddings, no token_type embeddings)\n",
        "        \"\"\"\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)  # (max_seq_length)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)  # (bs, max_seq_length)\n",
        "\n",
        "        word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
        "        position_embeddings = self.position_embeddings(position_ids)  # (bs, max_seq_length, dim)\n",
        "\n",
        "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
        "        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n",
        "        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = config.n_heads\n",
        "        self.dim = config.dim\n",
        "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
        "\n",
        "        assert self.dim % self.n_heads == 0\n",
        "\n",
        "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
        "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
        "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
        "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
        "\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        attention_head_size = self.dim // self.n_heads\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)\n",
        "        # Prune linear layers\n",
        "        self.q_lin = prune_linear_layer(self.q_lin, index)\n",
        "        self.k_lin = prune_linear_layer(self.k_lin, index)\n",
        "        self.v_lin = prune_linear_layer(self.v_lin, index)\n",
        "        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n",
        "        # Update hyper params\n",
        "        self.n_heads = self.n_heads - len(heads)\n",
        "        self.dim = attention_head_size * self.n_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, query, key, value, mask, head_mask=None, output_attentions=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        query: torch.tensor(bs, seq_length, dim)\n",
        "        key: torch.tensor(bs, seq_length, dim)\n",
        "        value: torch.tensor(bs, seq_length, dim)\n",
        "        mask: torch.tensor(bs, seq_length)\n",
        "\n",
        "        Outputs\n",
        "        -------\n",
        "        weights: torch.tensor(bs, n_heads, seq_length, seq_length)\n",
        "            Attention weights\n",
        "        context: torch.tensor(bs, seq_length, dim)\n",
        "            Contextualized layer. Optional: only if `output_attentions=True`\n",
        "        \"\"\"\n",
        "        bs, q_length, dim = query.size()\n",
        "        k_length = key.size(1)\n",
        "        # assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)\n",
        "        # assert key.size() == value.size()\n",
        "\n",
        "        dim_per_head = self.dim // self.n_heads\n",
        "\n",
        "        mask_reshp = (bs, 1, 1, k_length)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\" separate heads \"\"\"\n",
        "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\" group heads \"\"\"\n",
        "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
        "\n",
        "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
        "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
        "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
        "\n",
        "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
        "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
        "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
        "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)\n",
        "\n",
        "        weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)\n",
        "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            weights = weights * head_mask\n",
        "\n",
        "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
        "        context = unshape(context)  # (bs, q_length, dim)\n",
        "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
        "\n",
        "        if output_attentions:\n",
        "            return (context, weights)\n",
        "        else:\n",
        "            return (context,)\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=config.dropout)\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n",
        "        self.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n",
        "        assert config.activation in [\"relu\", \"gelu\"], \"activation ({}) must be in ['relu', 'gelu']\".format(\n",
        "            config.activation\n",
        "        )\n",
        "        self.activation = gelu if config.activation == \"gelu\" else nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
        "\n",
        "    def ff_chunk(self, input):\n",
        "        x = self.lin1(input)\n",
        "        x = self.activation(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config.dim % config.n_heads == 0\n",
        "\n",
        "        self.attention = MultiHeadSelfAttention(config)\n",
        "        self.sa_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
        "\n",
        "        self.ffn = FFN(config)\n",
        "        self.output_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
        "\n",
        "    def forward(self, x, attn_mask=None, head_mask=None, output_attentions=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.tensor(bs, seq_length, dim)\n",
        "        attn_mask: torch.tensor(bs, seq_length)\n",
        "\n",
        "        Outputs\n",
        "        -------\n",
        "        sa_weights: torch.tensor(bs, n_heads, seq_length, seq_length)\n",
        "            The attention weights\n",
        "        ffn_output: torch.tensor(bs, seq_length, dim)\n",
        "            The output of the transformer block contextualization.\n",
        "        \"\"\"\n",
        "        # Self-Attention\n",
        "        sa_output = self.attention(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            mask=attn_mask,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        if output_attentions:\n",
        "            sa_output, sa_weights = sa_output  # (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\n",
        "        else:  # To handle these `output_attentions` or `output_hidden_states` cases returning tuples\n",
        "            assert type(sa_output) == tuple\n",
        "            sa_output = sa_output[0]\n",
        "        sa_output = self.sa_layer_norm(sa_output + x)  # (bs, seq_length, dim)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(sa_output)  # (bs, seq_length, dim)\n",
        "        ffn_output = self.output_layer_norm(ffn_output + sa_output)  # (bs, seq_length, dim)\n",
        "\n",
        "        output = (ffn_output,)\n",
        "        if output_attentions:\n",
        "            output = (sa_weights,) + output\n",
        "        return output\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.n_layers = config.n_layers\n",
        "\n",
        "        layer = TransformerBlock(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.n_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self, x, attn_mask=None, head_mask=None, output_attentions=False, output_hidden_states=False, return_dict=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.tensor(bs, seq_length, dim)\n",
        "            Input sequence embedded.\n",
        "        attn_mask: torch.tensor(bs, seq_length)\n",
        "            Attention mask on the sequence.\n",
        "\n",
        "        Outputs\n",
        "        -------\n",
        "        hidden_state: torch.tensor(bs, seq_length, dim)\n",
        "            Sequence of hiddens states in the last (top) layer\n",
        "        all_hidden_states: Tuple[torch.tensor(bs, seq_length, dim)]\n",
        "            Tuple of length n_layers with the hidden states from each layer.\n",
        "            Optional: only if output_hidden_states=True\n",
        "        all_attentions: Tuple[torch.tensor(bs, n_heads, seq_length, seq_length)]\n",
        "            Tuple of length n_layers with the attention weights from each layer\n",
        "            Optional: only if output_attentions=True\n",
        "        \"\"\"\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        hidden_state = x\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_state,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                x=hidden_state, attn_mask=attn_mask, head_mask=head_mask[i], output_attentions=output_attentions\n",
        "            )\n",
        "            hidden_state = layer_outputs[-1]\n",
        "\n",
        "            if output_attentions:\n",
        "                assert len(layer_outputs) == 2\n",
        "                attentions = layer_outputs[0]\n",
        "                all_attentions = all_attentions + (attentions,)\n",
        "            else:\n",
        "                assert len(layer_outputs) == 1\n",
        "\n",
        "        # Add last layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_state,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_state, all_hidden_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_state, hidden_states=all_hidden_states, attentions=all_attentions\n",
        "        )\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0QxY_xYt9ij"
      },
      "source": [
        "# Distilbert basic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc4GWyLpuEPO"
      },
      "source": [
        "# INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #\n",
        "class DistilBertPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"An abstract class to handle weights initialization and\n",
        "    a simple interface for downloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = DistilBertConfig\n",
        "    load_tf_weights = None\n",
        "    base_model_prefix = \"distilbert\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights.\"\"\"\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            if module.weight.requires_grad:\n",
        "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "DISTILBERT_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__ subclass.\n",
        "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general\n",
        "    usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.DistilBertConfig`): Model configuration class with all the parameters of the model.\n",
        "            Initializing with a config file does not load the weights associated with the model, only the configuration.\n",
        "            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n",
        "\"\"\"\n",
        "\n",
        "DISTILBERT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.DistilBertTokenizer`.\n",
        "            See :meth:`transformers.PreTrainedTokenizer.encode` and\n",
        "            :meth:`transformers.PreTrainedTokenizer.__call__` for details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class DistilBertModel(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.embeddings = Embeddings(config)  # Embeddings\n",
        "        self.transformer = Transformer(config)  # Encoder\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"Prunes heads of the model.\n",
        "        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "        See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.transformer.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(input_shape, device=device)  # (bs, seq_length)\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)\n",
        "        return self.transformer(\n",
        "            x=inputs_embeds,\n",
        "            attn_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsoRPO8puE3r"
      },
      "source": [
        "# Distilbert For Token Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4HOHERJtyZ7"
      },
      "source": [
        "class DistilBertForTokenClassification(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[1:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9N4MbV30FYF"
      },
      "source": [
        "# Get Distilbert for Token Classification model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PM6Bqv90Ewu",
        "outputId": "c0cf0090-6394-40f8-c7ab-11fc307e610c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from transformers import DistilBertForTokenClassification\n",
        "\n",
        "unique_tags = {\n",
        " 'I-person',\n",
        " 'B-person',\n",
        " 'I-location',\n",
        " 'I-group',\n",
        " 'I-creative-work',\n",
        " 'B-location',\n",
        " 'O',\n",
        " 'B-corporation',\n",
        " 'I-corporation',\n",
        " 'I-product',\n",
        " 'B-product',\n",
        " 'B-creative-work',\n",
        " 'B-group'\n",
        "}\n",
        "\n",
        "model = DistilBertForTokenClassification.from_pretrained(PRE_TRAINED_BERT_MODEL, num_labels=len(unique_tags))\n",
        "\n",
        "torch.save(model.state_dict(), PRE_TRAINED_BERT_MODEL + '_weight.pth')\n",
        "\n",
        "del model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AjNHs58ECv0"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ1y8VlSEECn",
        "outputId": "a52a788d-2f8a-40b1-d818-789efcbed31b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vcwgb4LECv2",
        "outputId": "33ed1d91-5492-416c-ef98-4f8271b03e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://noisy-text.github.io/2017/files/wnut17train.conll"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-04 22:37:21--  http://noisy-text.github.io/2017/files/wnut17train.conll\n",
            "Resolving noisy-text.github.io (noisy-text.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to noisy-text.github.io (noisy-text.github.io)|185.199.108.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 493781 (482K) [application/octet-stream]\n",
            "Saving to: ‘wnut17train.conll.3’\n",
            "\n",
            "\rwnut17train.conll.3   0%[                    ]       0  --.-KB/s               \rwnut17train.conll.3 100%[===================>] 482.21K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-11-04 22:37:21 (14.1 MB/s) - ‘wnut17train.conll.3’ saved [493781/493781]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl2WEc8zECv8"
      },
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def read_wnut(file_path):\n",
        "    file_path = Path(file_path)\n",
        "\n",
        "    raw_text = file_path.read_text().strip()\n",
        "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for line in doc.split('\\n'):\n",
        "            token, tag = line.split('\\t')\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "\n",
        "    return token_docs, tag_docs\n",
        "\n",
        "texts, tags = read_wnut('wnut17train.conll')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsnoh5HmECwA",
        "outputId": "d1ce5763-c434-4bbd-c892-278f1cd57fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(set([str(t) for line in tags for t in line]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I-product',\n",
              " 'I-person',\n",
              " 'O',\n",
              " 'I-creative-work',\n",
              " 'B-group',\n",
              " 'B-person',\n",
              " 'B-corporation',\n",
              " 'I-location',\n",
              " 'B-product',\n",
              " 'B-location',\n",
              " 'I-corporation',\n",
              " 'B-creative-work',\n",
              " 'I-group']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCVoSsOIECwE",
        "outputId": "a4e0362f-c790-4e93-94ba-caf36ffb88d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
            "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTr96XeECwG"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e13IwtJzECwJ",
        "outputId": "748122c6-2266-4a8a-cd6f-76110ac0ef32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_texts), len(val_texts)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUwIwdr8ECwM"
      },
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs44h6xyECwO",
        "outputId": "09c718e1-ee65-4421-9830-d6c818d92020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "id2tag"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'I-product',\n",
              " 1: 'I-person',\n",
              " 2: 'O',\n",
              " 3: 'I-creative-work',\n",
              " 4: 'B-group',\n",
              " 5: 'B-person',\n",
              " 6: 'B-corporation',\n",
              " 7: 'I-location',\n",
              " 8: 'B-product',\n",
              " 9: 'B-location',\n",
              " 10: 'I-corporation',\n",
              " 11: 'B-creative-work',\n",
              " 12: 'I-group'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2jIJICiECwQ"
      },
      "source": [
        "# Use pre-trained DistilBert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxgof0hcECwQ"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_BERT_MODEL)\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SzEbadHECwS",
        "outputId": "a0eea1ac-5756-43f4-b523-104491777d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encodings.keys()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'offset_mapping'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIAibb_vECwU",
        "outputId": "6724cd2a-9dd6-47fc-d764-1589f7665bdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_encodings['input_ids']), len(val_encodings['input_ids'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dax_NmvvECwW",
        "outputId": "ead4bdd5-0d6b-4cc3-eeb7-fc9810c03bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_encodings['input_ids'][0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FxJ8bDnECwZ"
      },
      "source": [
        "# Format labels for sub token splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMIN-NmWECwZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, encodings):\n",
        "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    encoded_labels = []\n",
        "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
        "        # create an empty array of -100\n",
        "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
        "        arr_offset = np.array(doc_offset)\n",
        "\n",
        "        # set labels whose first offset position is 0 and the second is not 0\n",
        "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
        "        encoded_labels.append(doc_enc_labels.tolist())\n",
        "\n",
        "    return encoded_labels\n",
        "\n",
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VED-SYcGECwb",
        "outputId": "223bc3d1-0808-4153-e64c-1d72facf855a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_labels), len(val_labels)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2715, 679)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWJULml-ECwd",
        "outputId": "560db7c8-fe62-4eef-dd73-e75946099bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(train_labels[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii3Bm9jBECwf"
      },
      "source": [
        "# Create Dataset loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i67IxZZXECwf"
      },
      "source": [
        "import torch\n",
        "\n",
        "class WNUTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
        "val_dataset = WNUTDataset(val_encodings, val_labels)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OSGqeUW0zOf"
      },
      "source": [
        "# Instantiate Model from PyTorch class module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjlIDyx83Bry"
      },
      "source": [
        "from transformers import BertConfig\n",
        "config = BertConfig.from_pretrained(PRE_TRAINED_BERT_MODEL, num_labels=len(unique_tags)) "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2E7erXs0zAp"
      },
      "source": [
        "model = DistilBertForTokenClassification(config)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55ZRJlO1AfL"
      },
      "source": [
        "# Load Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7t8eZj61Ast"
      },
      "source": [
        "weight = torch.load(PRE_TRAINED_BERT_MODEL + '_weight.pth', map_location='cpu')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6MVFucy2Bar",
        "outputId": "8e51a2a7-1803-4d65-a186-9c8233f6fc9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.load_state_dict(weight)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE_Xz_HaECwk"
      },
      "source": [
        "# Fine Tune DistilBert using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEqQDeUUECwk",
        "outputId": "57284157-11ba-41fd-e234-b1d8ee0070a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertForSequenceClassification, AdamW\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(10):\n",
        "    epoch_loss = 0.0\n",
        "    num_batch = 0\n",
        "    for i, batch in enumerate(train_loader, 0):\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #print(f\"Epoch {epoch} Batch {i} : Loss {loss.data}\")\n",
        "        epoch_loss += loss.item()\n",
        "        num_batch += 1\n",
        "    print(f\"Epoch {epoch} : Loss {epoch_loss / num_batch}\")\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 : Loss 0.26349000037812137\n",
            "Epoch 1 : Loss 0.11957117296097909\n",
            "Epoch 2 : Loss 0.06412502633654238\n",
            "Epoch 3 : Loss 0.03327845499083838\n",
            "Epoch 4 : Loss 0.019229101187974103\n",
            "Epoch 5 : Loss 0.010307584674207166\n",
            "Epoch 6 : Loss 0.008896997650770252\n",
            "Epoch 7 : Loss 0.006020924705936683\n",
            "Epoch 8 : Loss 0.004160765742776943\n",
            "Epoch 9 : Loss 0.004878450673234457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjIY3GyqS9bN"
      },
      "source": [
        "# Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDb-iNphECwn"
      },
      "source": [
        "testloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de32VjIDTBaP"
      },
      "source": [
        "predictions = []\n",
        "actual = []\n",
        "with torch.no_grad():\n",
        "   for i, batch in enumerate(testloader, 0):\n",
        "     input_ids = batch['input_ids'].to(device)\n",
        "     attention_mask = batch['attention_mask'].to(device)\n",
        "     outputs = model(input_ids, attention_mask)\n",
        "     predictions.append(outputs[0].cpu())\n",
        "     actual.append(batch['labels'])\n",
        "predictions = np.concatenate(predictions)\n",
        "actual = np.concatenate(actual)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vt3pDcUTBf_"
      },
      "source": [
        "predicted_tokens = np.argmax(predictions, axis=2)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzcQirVlTBq3"
      },
      "source": [
        "predicted_tags = []\n",
        "\n",
        "for p_tokens, a_tokens in zip(predicted_tokens, actual):\n",
        "  a_mask = a_tokens!=-100\n",
        "  word_tokens = p_tokens[a_mask]\n",
        "  iob_tags = [id2tag[x] for x in word_tokens]\n",
        "  predicted_tags.append(iob_tags)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5sbg_58TCUC",
        "outputId": "7d4ae18a-b87d-454e-9783-6f3ed253966a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (1.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMmTRcGTBVh"
      },
      "source": [
        "from seqeval.metrics import f1_score"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lH2-QCiPECwp",
        "outputId": "f3db73a3-60d4-40b8-e995-67c5abf72be9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f1_score(val_tags, predicted_tags)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5379665379665378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}